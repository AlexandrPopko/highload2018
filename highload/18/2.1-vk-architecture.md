<p align="right"><img src = "images/tg-logo.png" width="20px" height=20px"> <a href = "https://t.me/docops">docops</a></p>

# FAQ по архитектуре и работе ВКонтакте

Алексей Акулович, ВКонтакте

## Содержание

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->


- [FAQ по архитектуре и работе ВКонтакте](#faq-%D0%BF%D0%BE-%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B5-%D0%B8-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B5-%D0%B2%D0%BA%D0%BE%D0%BD%D1%82%D0%B0%D0%BA%D1%82%D0%B5)
- [Архитектура](#%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0)
- [Базы данных](#%D0%B1%D0%B0%D0%B7%D1%8B-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85)
- [Логи](#%D0%BB%D0%BE%D0%B3%D0%B8)
- [Мониторинг](#%D0%BC%D0%BE%D0%BD%D0%B8%D1%82%D0%BE%D1%80%D0%B8%D0%BD%D0%B3)
- [Деплой](#%D0%B4%D0%B5%D0%BF%D0%BB%D0%BE%D0%B9)
- [Другие доклады про архитектуру VK](#%D0%B4%D1%80%D1%83%D0%B3%D0%B8%D0%B5-%D0%B4%D0%BE%D0%BA%D0%BB%D0%B0%D0%B4%D1%8B-%D0%BF%D1%80%D0%BE-%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D1%83-vk)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->


# Архитектура

* фронты: независимые сервера с nginx, анонсируют общие IP,терминируют HTTPS/WSS.
* бэкенды: http сервера на kPHP, модель работы prefork, вместо перезапуска сбрасывают состояние (global/static vars).

  Для распределения нагрузки:
  
    * Бэкенды группируются: general, mobile, api и т.п. Выбирает между ними nginx на фронте.
  
    * Сбор метрик и перебалансировка.

* Content server (cs). Хранят и обрабатывают файлы.
* Часть cs закрыты специальными pu/pp серверами (photo upload / photo proxy). Зачем они нужны?

    * для терминирования HTTPS
    * чтобы использовать серые IP-адреса для cs
    * для отказоустойчивости через общие IP
    * спорно — чтобы клиент держал меньше соединений

    Обычно видео гоняется напрямую, а более лёгкий контент — через прокси.
    
* sun — новая альтернатива pp. Зачем:

    * у pp один IP на группу, в результате один файл оседает во всех кешах
    * pp нельзя шардировать и ставить в регионах

  Как устроены sun:
  
    * маршрутизация anycast
    * кеширование
    * поддержка весов
    * можно ставить в регионах
    * шардирование по id контента (например, когда 100000 человек запрашивают аватарку одного пользователя)

* cache — региональные кеши. Регион определяем так:

    1. сбор сетей региона по BGP
    1. инфа загружается в базу, + geoip
    1. по IP пользователя определяем регион

* engines — базы данных

# Базы данных

Называем engines, потому что это не совсем базы данных.

В 2008-2009 использовали MySQL и Memcached, но они не выдержали взрывного роста пользователей. Заменили их на велосипеды.

Типов движков очень много. На каждую задачу — новый тип движка. Очереди, списки, сеты — всё что угодно.

Движки одного типа объединяются в кластеры. Код не знает расположения и размера кластеров. Для этого между серверами и базами есть ещё rpc-proxy:

* общая шина
* service discovery, forwarding
* circuit breaker

На каждом сервере есть локальный rpc-proxy, который знает, куда направить запросы и где находятся engines.

Если один engine идёт в другой, то тоже делает это через прокси. Engine не должен знать ничего, кроме себя.

Персистентное хранение данных:

* движки пишут бинлоги: binary log (WAL, AOF). Пишутся в одинаковом бинарном формате (TL scheme), чтобы админы их читали своими инструментами.
* снапшоты: слепок данных + offset в бинлоге. Общее начало, тело произвольное.

При перезапуске движок сначала читает снапшот, восстанавливает из него своё состояние. Потом из него находит offset, по нему дочитывает остаток из бинлога, восстанавливает окончательное состояние.

Результат: репликация данных:

* statement-based
* инкрементальный унос «хвоста» бинлога

Эта же схема используется для создания бэкапов.

![](images/vk-binlog.png)

# Логи

Как собираются?

* отправка в memcached. Там кольцевой буфер (`ring-buffer: prefix.idx = line`).
* отправка в logs-engine (разработан in-house)

Хранятся в ClickHouse. Чтобы это заработало, приходится локальный rpc-proxy заменить на KittenHouse, а на движке добавлять KittenHouse reverse proxy.

А ещё есть nginx, чтобы получать логи по UDP.

![](images/clickhouse-logs.png)

# Мониторинг

Есть два типа метрик.

Системные и админские метрики:

* netdata собирает статистику,
* отсылает в Graphite Carbon
* ClickHouse
* можно смотреть через Grafana

Продуктовые и разработческие метрики:

* много метрик
* очень много событий — от 0,6 до 1 триллиона в сутки
* храним 2+ года

Эксперимент: собираем метрики на ClickHouse

* удобнее основной системы
* требует меньше серверов, но сами сервера жирнее

# Деплой

Git, GitLab, TeamCity

PHP:

1. git pruduction branch
1. diff файла
1. записывается в binlog copyfast
1. реплицируется на сервера через gossip replication
1. применяется локальными репликами на локальной файловой системе

kPHP:

* большой бинарь, сотни МБ
* git master branch
* версию пишем в binlog copyfast
* реплицируем версию на сервера
* сервер вытягивает свежий бинарник через gossip replication
* graceful-перезапуск на новую версию

Движки:

* бинари в .deb
* git master branch
* версию пишем в binlog copyfast
* реплицируем версию на сервера
* сервер вытягивает свежий .deb
* dpkg -i 
* graceful-перезапуск на новую версию

# Другие доклады про архитектуру VK

* [Системный администратор ВКонтакте. Как?](https://highload.ru/2016/abstracts/2416)
* [Разработка в ненадёжной нагруженной среде](https://2018.codefest.ru/lecture/1250)
* Как VK вставляет данные в ClickHouse с десятков тысяч серверов: на [Highload Siberia](https://highload.ru/siberia/2018/abstracts/3614) и на [Highload Moscow](https://highload.ru/moscow/2018/abstracts/4066)
* [Архитектура растущего проекта на примере ВКонтакте](https://highload.ru/2016/abstracts/2414)
